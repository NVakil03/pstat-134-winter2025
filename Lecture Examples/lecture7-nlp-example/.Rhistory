count(id, bigram) %>%
bind_tf_idf(bigram, id, n) %>%
arrange(desc(tf_idf))
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 50) %>%
graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
bigram_graph
email_bigrams <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
email_bigrams <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
email_bigrams <- email_bigrams %>%
unite(bigram, word1, word2, sep = " ")
email_bigrams
email_bigrams %>%
count(id, bigram) %>%
bind_tf_idf(bigram, id, n) %>%
arrange(desc(tf_idf))
email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2)
email_data_for_later
email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2)
email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% arrange(desc(n))
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 15) %>%
graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 10) %>%
graph_from_data_frame()
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 10) %>%
graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 12) %>%
graph_from_data_frame()
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
email_data
sgram_model = word2vec(x = email_data, type = "skip-gram",
hs = FALSE, dim = 50, iter = 10,
sample = 0.01)
email_data
email_data %>% group_by(word) %>%
reframe(counts = n(), across(everything()))
email_data %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
# filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0)
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
# filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0)
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0)
email_tf_idf
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything()))
email_tf_idf <- email_tf_idf[1:1000,]
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
# filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0)
?pivot_wider
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "check_unique")
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
# filter(counts > 5) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "check_unique")
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "check_unique")
library(tidymodels)
library(tidyverse)
library(janitor)
library(reshape2)
library(wordcloud)
library(ggraph)
library(tidytext)
library(httr)
library(igraph)
library(data.table)
library(textdata)
library(ggplot2)
library(ggrepel)
library(plotly)
library(umap)
library(word2vec)
library(tm)
email_data <- read_csv("data/email.csv") %>%
clean_names()
email_data$id <- seq.int(nrow(email_data))
# removing HTML tags, replacing with a space
email_data$message <- str_replace_all(email_data$message, pattern = "<.*?>", " ")
# removing "\n", replacing with a space
email_data$message <- str_replace_all(email_data$message, pattern = "\n", " ")
# removing "&amp;" and "&gt;"
email_data$message <- str_replace_all(email_data$message, pattern = "&amp;", " ")
email_data$message <- str_replace_all(email_data$message, pattern = "&gt;", " ")
remove <- c('\n',
'[[:punct:]]',
'nbsp',
'[[:digit:]]',
'[[:symbol:]]',
'^br$',
'href',
'ilink') %>%
paste(collapse = '|')
# removing any other weird characters,
# any backslashes, adding space before capital
# letters and removing extra whitespace,
# replacing capital letters with lowercase letters
email_data$message <- email_data$message %>%
str_remove_all('\'') %>%
str_replace_all(remove, ' ') %>%
str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
tolower() %>%
str_replace_all("\\s+", " ")
email_data$message[5:10]
email_data_for_later <- email_data
data("stop_words")
email_data <- email_data %>%
unnest_tokens(word, message) %>%
anti_join(stop_words)
email_data %>%
count(word, sort = TRUE)
email_data %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
email_data %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "blue"),
max.words = 100)
email_tf_idf <- email_data %>%
count(id, word) %>%
bind_tf_idf(term = word,
document = id,
n = n)
email_tf_idf %>%
arrange(desc(tf_idf))
email_bigrams <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
email_bigrams <- email_bigrams %>%
unite(bigram, word1, word2, sep = " ")
email_bigrams %>%
count(bigram, sort = TRUE)
email_bigrams %>%
count(id, bigram) %>%
bind_tf_idf(bigram, id, n) %>%
arrange(desc(tf_idf))
bigram_graph <- email_data_for_later %>%
unnest_tokens(bigram, message, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2) %>% filter(n > 12) %>%
graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "check_unique")
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "check_unique")
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(id_cols = id,
names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "unique")
email_tf_idf %>% group_by(word) %>%
reframe(counts = n(), across(everything())) %>%
filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "unique")
email_tf_idf %>% group_by(word) %>%
# reframe(counts = n(), across(everything())) %>%
# filter(counts > 3) %>%
# left_join(okcupid_for_later) %>%
pivot_wider(names_from = word,
values_from = tf_idf,
values_fill = 0,
names_repair = "unique")
email_tf_idf %>% group_by(word)
corpus <- Corpus(VectorSource(email_data_for_later$message))
corpus
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
dtm <- DocumentTermMatrix(corpus)
dtm_matrix <- as.matrix(dtm)
dtm_matrix[1:5,1:12]
labels <- as.numeric(factor(email_data_for_later$category, levels = c("ham", "spam")))
labels <- labels - 1 # so that the levels are 0 and 1, not 1 and 2
dtm <- DocumentTermMatrix(corpus)
dtm_matrix <- as.matrix(dtm)
dtm_df <- as.data.table(dtm_matrix)
dtm_df$label <- labels
dtm_df
dtm_df <- dtm_df %>%
mutate(label = factor(label))
dtm_df
okcupid_split <- initial_split(dtm_df, strata = label)
okcupid_train <- training(okcupid_split)
email_train <- training(email_split)
email_split <- initial_split(dtm_df, strata = label)
email_train <- training(email_split)
email_test <- testing(email_split)
email_folds <- vfold_cv(email_train, v = 5, strata = label)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_zv(all_predictors())
prep(email_recipe) %>% bake(email_train) %>%
head()
prep(email_recipe) %>% bake(email_train)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_nzv(all_predictors())
prep(email_recipe) %>% bake(email_train)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_zv(all_predictors())
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification") %>%
set_engine("LiblineaR")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
install.packages("LiblineaR")
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification") %>%
set_engine("LiblineaR")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
show_best(svmlin_fit, metric = "accuracy")
prep(email_recipe) %>% bake(email_train) %>% select(label, everything())
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification", cost = 1, margin = 0.1) %>%
set_engine("LiblineaR")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
library(naniar)
email_data %>% vis_miss()
email_data_for_later %>% vis_miss()
prep(email_recipe) %>% bake(email_train) %>% select(label, everything()) %>% str()
prep(email_recipe) %>% bake(email_train) %>% select(label, everything()) %>% summary()
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_nzv(all_predictors())
prep(email_recipe) %>% bake(email_train) %>% select(label, everything()) %>% summary()
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification", cost = 1, margin = 0.1) %>%
set_engine("LiblineaR")
dim(email_data_for_later)
dim(email_data_for_later %>% drop_na())
email_data_for_later$category %>% levels()
email_data_for_later$category %>% factor() %>% table()
email_data_for_later$category %>% table()
email_data_for_later <- email_data_for_later %>%
filter(category %in% c("ham", "spam"))
email_data_for_later
corpus <- Corpus(VectorSource(email_data_for_later$message))
corpus
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stripWhitespace)
labels <- as.numeric(factor(email_data_for_later$category, levels = c("ham", "spam")))
labels <- labels - 1 # so that the levels are 0 and 1, not 1 and 2
dtm <- DocumentTermMatrix(corpus)
dtm_matrix <- as.matrix(dtm)
dtm_df <- as.data.table(dtm_matrix)
dtm_df <- as.data.table(dtm_matrix)
dtm_df$label <- labels
dtm_df <- dtm_df %>%
mutate(label = factor(label))
email_split <- initial_split(dtm_df, strata = label)
email_train <- training(email_split)
email_test <- testing(email_split)
email_folds <- vfold_cv(email_train, v = 5, strata = label)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_zv(all_predictors())
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification", cost = 1, margin = 0.1) %>%
set_engine("LiblineaR")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
email_data_for_later <- email_data_for_later %>%
filter(category %in% c("ham", "spam"))
corpus <- Corpus(VectorSource(email_data_for_later$message))
corpus
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
labels <- as.numeric(factor(email_data_for_later$category, levels = c("ham", "spam")))
labels <- labels - 1 # so that the levels are 0 and 1, not 1 and 2
dtm <- DocumentTermMatrix(corpus)
dtm_matrix <- as.matrix(dtm)
dtm_matrix[1:5,1:12]
dtm_df <- as.data.table(dtm_matrix)
dtm_df$label <- labels
# dtm_df <- dtm_df %>%
#   mutate(label = factor(label))
email_split <- initial_split(dtm_df, strata = label)
email_train <- training(email_split)
email_test <- testing(email_split)
email_folds <- vfold_cv(email_train, v = 5, strata = label)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_zv(all_predictors())
prep(email_recipe) %>% bake(email_train) %>% select(label, everything()) %>% summary()
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification", cost = 1, margin = 0.1) %>%
set_engine("LiblineaR")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
show_best(svmlin_fit, metric = "accuracy")
email_data_for_later <- email_data_for_later %>%
filter(category %in% c("ham", "spam"))
corpus <- Corpus(VectorSource(email_data_for_later$message))
corpus
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
labels <- as.numeric(factor(email_data_for_later$category, levels = c("ham", "spam")))
labels <- labels - 1 # so that the levels are 0 and 1, not 1 and 2
dtm <- DocumentTermMatrix(corpus)
dtm_matrix <- as.matrix(dtm)
dtm_matrix[1:5,1:12]
dtm_df <- as.data.table(dtm_matrix)
dtm_df$label <- labels
dtm_df <- dtm_df %>%
mutate(label = factor(label))
email_split <- initial_split(dtm_df, strata = label)
email_train <- training(email_split)
email_test <- testing(email_split)
email_folds <- vfold_cv(email_train, v = 5, strata = label)
email_recipe <- recipe(label ~ ., data = email_train) %>%
step_zv(all_predictors())
prep(email_recipe) %>% bake(email_train) %>% select(label, everything()) %>% summary()
# lasso_reg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
#   set_mode("classification") %>%
#   set_engine("glmnet")
svmlin <- svm_linear(mode = "classification", cost = 1, margin = 0.1) %>%
set_engine("kernlab")
svmlin_wkflow <- workflow() %>%
add_model(svmlin) %>%
add_recipe(email_recipe)
svmlin_fit <- fit_resamples(svmlin_wkflow, email_folds)
show_best(svmlin_fit, metric = "accuracy")
show_best(svmlin_fit, metric = "roc_auc")
email_data
reticulate::repl_python()
